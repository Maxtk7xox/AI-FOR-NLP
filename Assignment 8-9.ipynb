{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 基础概念复习\n",
    "机器学习方法主要用在什么特点的场景下？\n",
    "\n",
    "\n",
    "Ans:当我们很难通过简单的数学假定建立模型时，我们可以通过机器学习建立模型\n",
    "\n",
    "提出 3 个你认为使用了机器学习方法的现实场景.\n",
    "\n",
    "\n",
    "Ans:人脸识别，siri，天气预报\n",
    "\n",
    "提出 3 个你认为可以使用机器学习但是还没有使用机器学习方法的场景.\n",
    "\n",
    "\n",
    "Ans:材料强度鉴定，地震预测，通过身体指数预防心理疾病\n",
    "\n",
    "\n",
    "什么是“模型”？ 为什么说“All models are wrong, but some useful”.\n",
    "\n",
    "\n",
    "Ans:对现实问题的抽象，我们的抽象过程不可能和实际情况一模一样，一定会存在一定程度的误差，但好的模型的结果有可以让我们参考的部分。\n",
    "\n",
    "Classification 和 Regressionu主要针对什么？ 有什么区别？\n",
    "\n",
    "\n",
    "Ans:classification是针对分类问题，regression是针对数值的预测。classification不会对数值进行预测，而是重点在将数据按照相似性进行分类，regression适用于我们需要具体数值作为结果时。\n",
    "\n",
    "precision， recall，f1, auc 分别是什么意思？ 假设一个城市有 10000 人，有 30 个犯罪分子，警察抓到了 35 个人，其中 20 个是犯罪分子，请问这个警察的 precision, recall, f1,auc 分别是什么？\n",
    "\n",
    "\n",
    "Ans:precision = 57.1% ; recall = 66.7% ; f1 = 0.615 ; auc是roc曲线下的面积\n",
    "\n",
    "请提出两种场景，第一种场景下，对模型的评估很注重 precision, 第二种很注重 recall.\n",
    "\n",
    "\n",
    "Ans:precision高的时候要保证false positive低，比如抓人，如果抓错一个人要赔100万，那我们就希望尽可能少抓错人。recall高的时候我们希望尽可能抓到了所有的人，比如如果抓不到这个人，他就会让我们损失1个亿，那我们就希望尽可能全部抓到。\n",
    "\n",
    "什么是 Overfitting， 什么是 Underfitting?\n",
    "\n",
    "\n",
    "Ans:overfitting是指模型对training set的拟合准确率远超过对test set的准确率，underfitting是指在training的过程中模型的准确率就不尽如人意。\n",
    "\n",
    "Lazy-Learning， Lazy在哪里？\n",
    "\n",
    "\n",
    "Ans:以knn为例，更多地是记录大量数据，并搜索，学习的部分不多\n",
    "\n",
    "Median， Mode， Mean分别是什么？ 有什么意义？\n",
    "\n",
    "\n",
    "Ans:分别是中位数，众数和平均数。结合三者能够判断数据的分布是否均匀，举例来说，如果平均数和众数大于中位数，那么数据分布就出现了skewed。\n",
    "\n",
    "Outlinear（异常值、离群值）是什么？ 如何定义？\n",
    "\n",
    "\n",
    "Ans:outlier指一组数据中与其他数据偏离较大的数据，outlier有多种定义方法，其中常用的一种是小于25percentile/1.5或者大于75percentile的1.5倍\n",
    "\n",
    "Bias 和 Variance 有什么关系？ 他们之间为什么是一种 tradeoff 的？\n",
    "\n",
    "\n",
    "Ans:bias和variance共同组成了我们的模型误差，bias的出现基于我们对模型有了错误的假设。如一组数据实际上满足二次函数的分布，我们用一次函数的模型进行拟合，就会有较大的bias，而variance体现在模型对训练集微小变化的敏感性上。 当我们增加模型的复杂度（自由度），会提高variance减小bias，相应地减小模型复杂度，会提高bias，减小variance。 \n",
    "\n",
    "Train， Validation，Test 数据集之间是什么关系？ 为什么要这么划分？\n",
    "\n",
    "\n",
    "Ans:training set用于建模， validation用于在建模中比较和分析模型， test set用于测试最后的结果。我们需要数据来让机器进行学习，学习过程中需要优化，因为有了train和validation，但是我们同时需要没被机器“见到”过的数据来检验模型对于新数据的学习能力，所以分出了test set。\n",
    "\n",
    "Supervised Learning 的 Supervised 体现在什么地方？\n",
    "\n",
    "\n",
    "Ans:被学习的变量是有值的，比如我们给垃圾邮件分类，我们会知道哪些是垃圾邮件。\n",
    "\n",
    "Linear Regression 中，什么是“线性关系”？\n",
    "\n",
    "\n",
    "Ans:自变量与因变量之间的关系是线性的\n",
    "\n",
    "Linear Regression中，Loss 函数怎么定义的？ 为什么要写成这样？ 什么是凸函数？ 优化中有什么意义？\n",
    "\n",
    "\n",
    "Ans:C=∑=1n(y−yˆ)2 在线性回归中我们将mean squared error定义为loss function，也就是预测值到实际值的距离的平方和，这样可以更好地描述预测值与实际值之间的关系，同时mse越小，表示模型拟合越好。凸函数是只有一个全局最低点的函数，在我们做梯度下降的时候，如果损失函数非凸，我们可能会陷入局部最优解，而如果损失函数是凸函数，就可以避免这个问题（但需注意学习速度）\n",
    "\n",
    "简述Gradient Descent的过程，以 $y = -10 * x^2 + 3x + 4 $ 为例，从一个任一点 $ x = 10 $ 开始，如果根据 Gradient Descent 找到最值。\n",
    "\n",
    "\n",
    "Ans:在x = 10 这一点将系数1，3带入损失函数的偏导数，如果求出的偏导数为负，那么我们增加相应的系数的值，反之减小对应系数的值。\n",
    "\n",
    "一般在机器学习数量时，会做一个预处理（Normalization）， 简述 Normalization 的过程，以及数据经过 Normalization之后的平均值和标准差的情况。\n",
    "\n",
    "\n",
    "Ans:将数据中的每个数减去平均值后除以标准差，数据经过normalization后平均值为0，标准差为1\n",
    "\n",
    "Logstic Regression 的 Logstic 是什么曲线，被用在什么地方？\n",
    "\n",
    "\n",
    "Ans:在0到1区间内单调递增的曲线。\n",
    "\n",
    "Logstic Regression 的 Loss 函数 Cross Entropy 是怎么样的形式？ 有什么意义？\n",
    "\n",
    "\n",
    "Ans:=−∑y′log(y) 因为log函数在x为1时值为0，所以当预测值和实际值相差越小的时候，cross entropy越小。我们可以由cross entropy的大小来比较模型的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/air/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('sqlResult_1558435.csv',encoding = 'gb18030')\n",
    "raw_data = raw_data[['content','source']]\n",
    "raw_data_xinhua = raw_data[raw_data['source'] == '新华社']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/air/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/air/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8391"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_other = raw_data[raw_data['source'] != '新华社']\n",
    "raw_data_other.dropna(axis=0,inplace = True)\n",
    "raw_data_other.loc[:,'source'] = 0\n",
    "len(raw_data_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string):\n",
    "    return ' '.join(jieba.cut(string))\n",
    "def token(string):\n",
    "    return re.findall(r'[\\w|\\d]+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8391"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.random.permutation(78661)\n",
    "index = index[:8391]\n",
    "index = list(index)\n",
    "raw_data_xinhua = raw_data_xinhua.iloc[index]\n",
    "len(raw_data_xinhua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_xinhua_content = raw_data_xinhua['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function for us that can remove '新华社' from our content\n",
    "def remove(string):\n",
    "    list1 = token(string)\n",
    "    list2 = [word for word in list1 if '新华社' not in word]\n",
    "    return ''.join(list2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'蒙巴萨肯尼亚2017年5月31日n国际6中国制造蒙内铁路通车n5月31日在肯尼亚蒙巴萨的蒙内铁路蒙巴萨西站两名服务人员迎候旅客n当地时间5月31日11时10分由中国企业承建的肯尼亚蒙巴萨内罗毕标轨铁路蒙内铁路首班列车发车标志着蒙内铁路正式建成通车nn'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing whether it works well\n",
    "remove(raw_xinhua_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_other = raw_data_other['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/5d/046t421j05g25t76d5lv813w0000gn/T/jieba.cache\n",
      "Loading model cost 0.888 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "raw_other = [remove(sentence) for sentence in raw_other]\n",
    "raw_other = [cut(sentence) for sentence in raw_other]\n",
    "other = [(content, 0) for content in raw_other]\n",
    "raw_other = pd.DataFrame(other, columns=['content','source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>蒙巴萨 肯尼亚 2017 年 5 月 31 日 n 国际 6 中国 制造 蒙内 铁路 通车 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>外代 2017 年 6 月 1 日 n 外代 二线 网球 法网 穆古 拉扎 胜康塔 维特 n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>记者 陆佳飞 周而捷 美国国会参议院 ２ ２ 日以 ８ ２ 票 赞成 １ ３ 票 反对 的 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>核桃 专家 潘学军 论文 写 在 乌蒙山 nnn 谷雨刚 过 贵阳市 花溪 大道 旁 的 贵...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>天津 2017 年 4 月 27 日 n 小 车间 里 的 大 工匠 n4 月 27 日 创...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  source\n",
       "0  蒙巴萨 肯尼亚 2017 年 5 月 31 日 n 国际 6 中国 制造 蒙内 铁路 通车 ...       1\n",
       "1  外代 2017 年 6 月 1 日 n 外代 二线 网球 法网 穆古 拉扎 胜康塔 维特 n...       1\n",
       "2  记者 陆佳飞 周而捷 美国国会参议院 ２ ２ 日以 ８ ２ 票 赞成 １ ３ 票 反对 的 ...       1\n",
       "3  核桃 专家 潘学军 论文 写 在 乌蒙山 nnn 谷雨刚 过 贵阳市 花溪 大道 旁 的 贵...       1\n",
       "4  天津 2017 年 4 月 27 日 n 小 车间 里 的 大 工匠 n4 月 27 日 创...       1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_xinhua_content = [remove(sentence) for sentence in raw_xinhua_content]\n",
    "raw_xinhua_content = [cut(sentence) for sentence in raw_xinhua_content]\n",
    "xinhua = [(content, 1) for content in raw_xinhua_content]\n",
    "raw_data_1 = pd.DataFrame(xinhua, columns=['content','source'])\n",
    "raw_data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content      object\n",
       "source     category\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_total = pd.concat([raw_data_1,raw_other])\n",
    "raw_data_total['source'] = raw_data_total['source'].astype('category')\n",
    "raw_data_total.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content,test_content,train_label,test_label = train_test_split(raw_data_total['content'],raw_data_total['source'],test_size = 0.25, random_state = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content = pd.concat([train_content, test_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    max_features=10000)\n",
    "word_vectorizer.fit(all_content)\n",
    "train_word_features = word_vectorizer.transform(train_content)\n",
    "test_word_features = word_vectorizer.transform(test_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9989786871402402"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = LogisticRegression()\n",
    "cv_score = np.mean(cross_val_score(logistic, train_word_features, train_label, cv=5, scoring='roc_auc'))\n",
    "cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9718779790276454"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_word_features,train_label).score(test_word_features,test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求问一下助教小姐姐。。这个哪里出了问题呢，怎么感觉准确率过高了。。新华社也去掉了 target也categorical了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
